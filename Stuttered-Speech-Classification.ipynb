{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Number of features: 80\n",
      "INFO:__main__:Array read from file datasetArray80.gz\n",
      "INFO:__main__:Neural network of depth 3\n",
      "INFO:tensorflow:Restoring parameters from models/2017-11-24-03:49:34-0.905953/session.ckpt\n",
      "INFO:__main__:Accuracy: 0.913870\n",
      "100% (3597 of 3597) |#####################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "progressbar.streams.wrap_stderr()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtraction:\n",
    "    def __init__(self, filename, n_mels=128):\n",
    "        try:\n",
    "            if not os.path.isfile(filename):\n",
    "                raise\n",
    "            self.filename = filename\n",
    "        except:\n",
    "            logger.error(\"%s does not exists or is not a file\", filename)\n",
    "            sys.exit()\n",
    "            \n",
    "        self.n_mels = n_mels\n",
    "        self.y = None\n",
    "        self.sr = None\n",
    "        self.S = None\n",
    "        self.log_S = None\n",
    "        self.mfcc = None\n",
    "        self.delta_mfcc = None\n",
    "        self.delta2_mfcc = None\n",
    "        self.M = None\n",
    "        self.rmse = None\n",
    "    \n",
    "    def loadFile(self):\n",
    "        self.y, self.sr = librosa.load(self.filename)\n",
    "        logger.debug('File loaded %s', self.filename)\n",
    "    \n",
    "    def melspectrogram(self):\n",
    "        self.S = librosa.feature.melspectrogram(self.y, sr=self.sr, n_mels=self.n_mels)\n",
    "        self.log_S = librosa.logamplitude(self.S, ref_power=np.max)\n",
    "    \n",
    "    def plotmelspectrogram(self):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        librosa.display.specshow(self.log_S, sr=self.sr, x_axis='time', y_axis='mel')\n",
    "        plt.title('mel Power Spectrogram')\n",
    "        plt.colorbar(format='%+02.0f dB')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    def extractmfcc(self, n_mfcc=13):\n",
    "        self.mfcc = librosa.feature.mfcc(S=self.log_S, n_mfcc=n_mfcc)\n",
    "        self.delta_mfcc = librosa.feature.delta(self.mfcc)\n",
    "        self.delta2_mfcc = librosa.feature.delta(self.mfcc, order=2)\n",
    "        self.M = np.vstack([self.mfcc, self.delta_mfcc, self.delta2_mfcc])\n",
    "    \n",
    "    def plotmfcc(self):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(3, 1, 1)\n",
    "        librosa.display.specshow(self.mfcc)\n",
    "        plt.ylabel('MFCC')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.subplot(3, 1, 2)\n",
    "        librosa.display.specshow(self.delta_mfcc)\n",
    "        plt.ylabel('MFCC-$\\Delta$')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.subplot(3, 1, 3)\n",
    "        librosa.display.specshow(self.delta2_mfcc, sr=self.sr, x_axis='time')\n",
    "        plt.ylabel('MFCC-$\\Delta^2$')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "    \n",
    "    def extractrmse(self):\n",
    "        self.rmse = librosa.feature.rmse(y=self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, datasetDir, datasetLabelFilename, datasetArrayFilename):\n",
    "        try:\n",
    "            if not os.path.isdir(datasetDir):\n",
    "                raise\n",
    "            self.datasetDir = datasetDir\n",
    "            logger.debug(\"Dataset Directory: %s\", self.datasetDir)\n",
    "        except:\n",
    "            logger.error(\"%s does not exists or is not a directory\", datasetDir)\n",
    "            sys.exit()\n",
    "        \n",
    "        try:\n",
    "            if not os.path.isfile(datasetLabelFilename):\n",
    "                raise\n",
    "            self.datasetLabelFilename = datasetLabelFilename\n",
    "            logger.debug(\"Dataset labels filename: %s\", self.datasetLabelFilename)\n",
    "        except:\n",
    "            logger.error(\"%s does not exists or is not a file\", datasetLabelFilename)\n",
    "            sys.exit()\n",
    "        \n",
    "        self.datasetArrayFilename = datasetArrayFilename\n",
    "        logger.debug(\"Dataset array filename: %s\", self.datasetArrayFilename)\n",
    "        \n",
    "        self.n_features = 80\n",
    "        logger.info(\"Number of features: %s\", self.n_features)\n",
    "        self.X = np.empty(shape=(0, self.n_features))\n",
    "        self.Y = np.empty(shape=(0, 2))\n",
    "        \n",
    "    \n",
    "    def build(self):\n",
    "        num_lines = sum(1 for line in open(self.datasetLabelFilename, 'r'))\n",
    "        with open(self.datasetLabelFilename, 'r') as datasetLabelFile:\n",
    "            filesProcessed=0\n",
    "            pbar = progressbar.ProgressBar(redirect_stdout=True)\n",
    "            for line in pbar(datasetLabelFile, max_value=num_lines):\n",
    "                lineSplit = line.strip().split(' ')\n",
    "                audiofilename = lineSplit[0]\n",
    "                label = lineSplit[1]\n",
    "                try:\n",
    "                    features = FeatureExtraction(os.path.join(self.datasetDir, audiofilename))\n",
    "                    features.loadFile()\n",
    "                    features.melspectrogram()\n",
    "                    features.extractmfcc()\n",
    "                    features.extractrmse()\n",
    "                except ValueError:\n",
    "                    logger.warning(\"Error extracting features from file %s\", audiofilename)\n",
    "                    continue\n",
    "                \n",
    "                featureVector = []\n",
    "                for feature in features.mfcc:\n",
    "                    featureVector.append(np.mean(feature))\n",
    "                    featureVector.append(np.var(feature))\n",
    "                \n",
    "                for feature in features.delta_mfcc:\n",
    "                    featureVector.append(np.mean(feature))\n",
    "                    featureVector.append(np.var(feature))\n",
    "                \n",
    "                for feature in features.delta2_mfcc:\n",
    "                    featureVector.append(np.mean(feature))\n",
    "                    featureVector.append(np.var(feature))\n",
    "                \n",
    "                featureVector.append(np.mean(features.rmse))\n",
    "                featureVector.append(np.var(features.rmse))\n",
    "                \n",
    "                self.X = np.vstack((self.X, [featureVector]))\n",
    "                \n",
    "                if label == \"STUTTER\":\n",
    "                    self.Y = np.vstack((self.Y, [0, 1]))\n",
    "                elif label == \"NORMAL\":\n",
    "                    self.Y = np.vstack((self.Y, [1, 0]))\n",
    "                else:\n",
    "                    logger.error(\"Unexpected label: %s\", label)\n",
    "                    sys.exit()\n",
    "                \n",
    "                filesProcessed += 1            \n",
    "            \n",
    "            logger.info(\"Total files processed: %d\", filesProcessed)\n",
    "    \n",
    "    def writeToFile(self, filename=None):\n",
    "        if filename == None:\n",
    "            filename = self.datasetArrayFilename\n",
    "            \n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        np.savetxt(filename, np.hstack((self.X, self.Y)))\n",
    "        logger.info(\"Array stored in file %s\", filename)\n",
    "    \n",
    "    def readFromFile(self, filename=None):\n",
    "        if filename == None:\n",
    "            filename = self.datasetArrayFilename\n",
    "            \n",
    "        if not os.path.isfile(filename):\n",
    "            logger.error(\"%s does not exists or is not a file\", filename)\n",
    "            sys.exit()\n",
    "        matrix = np.loadtxt(filename)\n",
    "        self.X = matrix[:, 0:self.n_features]\n",
    "        self.Y = matrix[:, self.n_features:]\n",
    "        logger.info(\"Array read from file %s\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('dataset', 'datasetLabels.txt', 'datasetArray80.gz')\n",
    "if not os.path.isfile(dataset.datasetArrayFilename):\n",
    "    dataset.build()\n",
    "    dataset.writeToFile()\n",
    "else:\n",
    "    dataset.readFromFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, X_train, Y_train, X_test, Y_test):\n",
    "        # Data\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        \n",
    "        # Learning Parameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.training_epochs = 3500\n",
    "        self.batch_size = 100\n",
    "        self.display_step = 100\n",
    "\n",
    "        # Model Parameters\n",
    "        self.n_hidden = [10, 10, 10]\n",
    "        self.hiddenLayers = len(self.n_hidden)\n",
    "        self.n_input = dataset.n_features\n",
    "        self.n_classes = 2\n",
    "\n",
    "        logger.info(\"Neural network of depth %d\", self.hiddenLayers)\n",
    "        for i in range(self.hiddenLayers):\n",
    "            logger.debug(\"Depth of layer %d is %d\", (i + 1), self.n_hidden[i])\n",
    "\n",
    "        self.x = tf.placeholder(\"float\", [None, self.n_input])\n",
    "        self.y = tf.placeholder(\"float\", [None, self.n_classes])\n",
    "        self.layer = None\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        # Model\n",
    "        self.model = self.network(self.x)\n",
    "        self.save_path = None\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.model, labels=self.y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        # Initialize the variables\n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def network(self, x):\n",
    "        self.layer = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for n_layer in range(self.hiddenLayers):\n",
    "            if n_layer == 0:\n",
    "                self.weights.append(tf.Variable(tf.random_normal([self.n_input, self.n_hidden[n_layer]])))\n",
    "                self.biases.append(tf.Variable(tf.random_normal([self.n_hidden[n_layer]])))\n",
    "                self.layer.append(tf.nn.relu(tf.add(tf.matmul(x, self.weights[n_layer]), self.biases[n_layer])))\n",
    "            else:\n",
    "                self.weights.append(tf.Variable(tf.random_normal([self.n_hidden[n_layer - 1], self.n_hidden[n_layer]])))\n",
    "                self.biases.append(tf.Variable(tf.random_normal([self.n_hidden[n_layer]])))\n",
    "                self.layer.append(tf.nn.relu(tf.add(tf.matmul(self.layer[n_layer - 1], self.weights[n_layer]), self.biases[n_layer])))\n",
    "\n",
    "\n",
    "        # Output layer\n",
    "        self.weights.append(tf.Variable(tf.random_normal([self.n_hidden[self.hiddenLayers - 1], self.n_classes])))\n",
    "        self.biases.append(tf.Variable(tf.random_normal([self.n_classes])))\n",
    "        self.layer.append(tf.matmul(self.layer[self.hiddenLayers - 1], self.weights[self.hiddenLayers]) + self.biases[self.hiddenLayers])\n",
    "\n",
    "        return self.layer[self.hiddenLayers]\n",
    "    \n",
    "    def train(self):\n",
    "        logger.info(\"Training the neural network\")\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(self.init)\n",
    "            pbar = progressbar.ProgressBar(redirect_stdout=True)\n",
    "            for epoch in pbar(range(self.training_epochs)):\n",
    "                avg_cost = 0\n",
    "                total_batch = int(len(self.X_train) / self.batch_size)\n",
    "                X_batches = np.array_split(self.X_train, total_batch)\n",
    "                Y_batches = np.array_split(self.Y_train, total_batch)\n",
    "\n",
    "                for i in range(total_batch):\n",
    "                    batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "                    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                    _, c = sess.run([self.optimizer, self.cost], feed_dict={self.x: batch_x, self.y: batch_y})\n",
    "\n",
    "                    # Compute average loss\n",
    "                    avg_cost += c / total_batch\n",
    "\n",
    "                # Display logs per epoch step\n",
    "                if epoch % self.display_step == 0:\n",
    "                    logger.debug(\"cost = %.9f\", avg_cost)\n",
    "            logger.info(\"Optimization Finished!\")\n",
    "\n",
    "            evalAccuracy = self.getAccuracy()\n",
    "            \n",
    "            global result \n",
    "            result = tf.argmax(self.model, 1).eval({self.x: X_test, self.y: Y_test})\n",
    "            \n",
    "            if not os.path.isdir(\"models\"):\n",
    "                os.makedirs(\"models\")\n",
    "            timestamp = '{:%Y-%m-%d-%H:%M:%S}'.format(datetime.datetime.now()) + '-' + str(evalAccuracy)\n",
    "            os.makedirs(os.path.join(\"models\", timestamp))\n",
    "            modelfilename =  os.path.join(os.path.join(\"models\", timestamp), 'session.ckpt')\n",
    "            self.save_path = saver.save(sess, modelfilename)\n",
    "            \n",
    "            with open(os.path.join(os.path.join(\"models\", timestamp), 'details.txt'), 'w') as details:\n",
    "                details.write(\"learning_rate = \" + str(self.learning_rate) + \"\\n\")\n",
    "                details.write(\"training_epochs = \" + str(self.training_epochs) + \"\\n\")\n",
    "                details.write(\"batch_size = \" + str(self.batch_size) + \"\\n\")\n",
    "                details.write(\"display_step = \" + str(self.display_step) + \"\\n\")\n",
    "                details.write(\"n_hidden = \" + str(self.n_hidden) + \"\\n\")\n",
    "                details.write(\"hiddenLayers = \" + str(self.hiddenLayers) + \"\\n\")\n",
    "                details.write(\"n_input = \" + str(self.n_input) + \"\\n\")\n",
    "                details.write(\"n_classes = \" + str(self.n_classes) + \"\\n\")\n",
    "                \n",
    "            logger.info(\"Model saved in file: %s\" % self.save_path)\n",
    "    \n",
    "    def getModelPath(self):\n",
    "        return self.save_path\n",
    "        \n",
    "    def getAccuracy(self):\n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(self.model, 1), tf.argmax(self.y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        evalAccuracy = accuracy.eval({self.x: self.X_test, self.y: self.Y_test})\n",
    "        logger.info(\"Accuracy: %f\", evalAccuracy)\n",
    "        return evalAccuracy\n",
    "        \n",
    "    def loadFromFile(self, filename):\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, filename)\n",
    "            self.getAccuracy()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset.X, dataset.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/2017-11-24-03:49:34-0.905953/session.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "nn = NeuralNetwork(X_train, Y_train, X_test, Y_test)\n",
    "#nn.train()\n",
    "nn.loadFromFile('models/2017-11-24-03:49:34-0.905953/session.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using the NN model for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('wav/release1/M_1106_25y0m_1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentLength = 250\n",
    "segmentHop = 50\n",
    "samplesPerSegment = int(segmentLength * sr / 1000)\n",
    "samplesToSkipPerHop = int(segmentHop * sr / 1000)\n",
    "upperLimit = int(len(y) - samplesPerSegment)\n",
    "\n",
    "pbar = progressbar.ProgressBar()\n",
    "for start in pbar(range(0, upperLimit, samplesToSkipPerHop)):\n",
    "    end = start + samplesPerSegment\n",
    "    audio = y[start:end]\n",
    "audio = y[end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
